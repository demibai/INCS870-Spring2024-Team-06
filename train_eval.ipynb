{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_selection import RFE, RFECV, VarianceThreshold, SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from configparser import ConfigParser\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = ConfigParser()\n",
    "constants.read(\"constants.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(pca=False):\n",
    "    train_set_path = os.path.join(constants.get(\"CONSTANTS\", \"DATASET_PATH\"), constants.get(\"CONSTANTS\", \"TRAIN_FILE\"))\n",
    "    test_set_path = os.path.join(constants.get(\"CONSTANTS\", \"DATASET_PATH\"), constants.get(\"CONSTANTS\", \"TEST_FILE\"))\n",
    "    train_data = pd.read_csv(train_set_path)\n",
    "    test_data = pd.read_csv(test_set_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    cnt = 0\n",
    "    for feature in data.columns:\n",
    "        if data[feature].dtype == \"float64\":\n",
    "            data[feature] = np.log1p(data[feature])\n",
    "            cnt += 1\n",
    "    print(\"[Normalization] Normalized\", cnt, \"numerical features.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pre-trained machine learning model stored in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_pickle(model_path):\n",
    "    model = joblib.load(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset<br>\n",
    "Transform original data type into a categorical data type.<br>\n",
    "Then encode the categories as integer codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_num(data):\n",
    "    # protocol\n",
    "    data[\"proto\"] = LabelEncoder().fit_transform(data[\"proto\"])\n\n",
    "    # service\n",
    "    data[\"service\"] = LabelEncoder().fit_transform(data[\"service\"])\n\n",
    "    # state\n",
    "    data[\"state\"] = LabelEncoder().fit_transform(data[\"state\"])\n\n",
    "    # attack category\n",
    "    data[\"attack_cat\"] = LabelEncoder().fit_transform(data[\"attack_cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x_train, y_train, x_test, method, k=None, cv=None):\n",
    "    # Feature selection sub-routine\n",
    "    feature_selection_time = 0\n",
    "    if method == \"rfe\" or method == \"rfecv\":\n",
    "        # Recursive Feature Elimination\n",
    "        if not k:\n",
    "            k = 20\n",
    "        else:\n",
    "            k = int(k)\n",
    "        print(\"[Feature Selection] Using Recursive Feature Elimination,\", \"selecting\", str(k), \"features.\")\n",
    "        start_time = time.time()\n",
    "        model = XGBClassifier()\n",
    "        if method == \"rfecv\":\n",
    "            cv = 5 if not cv else int(cv)\n",
    "            print(\"[Feature Selection] Using RFECV, splitting dataset into %d folds.\" % cv)\n",
    "            rfe = RFECV(model, step=1, min_features_to_select=k,cv=cv)\n",
    "        else:\n",
    "            rfe = RFE(model, n_features_to_select=k)\n",
    "        fit = rfe.fit(x_train, y_train)\n",
    "        selected_features = fit.get_support(indices=True)\n",
    "        x_train = x_train[x_train.columns[selected_features]]\n",
    "        x_test = x_test[x_test.columns[selected_features]]\n",
    "        end_time = time.time()\n",
    "        feature_selection_time = round(end_time - start_time, 2)\n",
    "        print(\"[Feature Selection] RFE took\", feature_selection_time, \"seconds\")\n",
    "    elif method == \"variance_threshold\":\n",
    "        if not k:\n",
    "            k = 0.0\n",
    "        print(\"[Feature Selection] Using Variance Threshold\", \"using\", str(k), \"as threshold.\")\n",
    "        start_time = time.time()\n",
    "        vt = VarianceThreshold(threshold=k)\n",
    "        fit = vt.fit(x_train, y_train)\n",
    "        selected_features = fit.get_support(indices=True)\n",
    "        x_train = x_train[x_train.columns[selected_features]]\n",
    "        x_test = x_test[x_test.columns[selected_features]]\n",
    "        end_time = time.time()\n",
    "        feature_selection_time = round(end_time - start_time, 2)\n",
    "        print(\"[Feature Selection] Variance Threshold took\", feature_selection_time, \"seconds\")\n",
    "    elif method == \"chi2\":\n",
    "        if not k:\n",
    "            k = 20\n",
    "        else:\n",
    "            k = int(k)\n",
    "        print(\"[Feature Selection] Using SelectKBest,\", \"selecting\", str(k), \"features.\")\n",
    "        start_time = time.time()\n",
    "        skb = SelectKBest(chi2, k=k)\n",
    "        fit = skb.fit(x_train, y_train)\n",
    "        selected_features = fit.get_support(indices=True)\n",
    "        x_train = x_train[x_train.columns[selected_features]]\n",
    "        x_test = x_test[x_test.columns[selected_features]]\n",
    "        end_time = time.time()\n",
    "        feature_selection_time = round(end_time - start_time, 2)\n",
    "        print(\"[Feature Selection] SelectKBest took\", feature_selection_time, \"seconds\")\n",
    "    elif method == \"anova\":\n",
    "        if not k:\n",
    "            k = 20\n",
    "        else:\n",
    "            k = int(k)\n",
    "        print(\"[Feature Selection] Using SelectKBest,\", \"selecting\", str(k), \"features.\")\n",
    "        start_time = time.time()\n",
    "        skb = SelectKBest(f_classif, k=k)\n",
    "        fit = skb.fit(x_train, y_train)\n",
    "        selected_features = fit.get_support(indices=True)\n",
    "        x_train = x_train[x_train.columns[selected_features]]\n",
    "        x_test = x_test[x_test.columns[selected_features]]\n",
    "        end_time = time.time()\n",
    "        feature_selection_time = round(end_time - start_time, 2)\n",
    "        print(\"[Feature Selection] SelectKBest took\", feature_selection_time, \"seconds\")\n",
    "    elif method == \"mutual_information\":\n",
    "        if not k:\n",
    "            k = 20\n",
    "        else:\n",
    "            k = int(k)\n",
    "        print(\"[Feature Selection] Using SelectKBest,\", \"selecting\", str(k), \"features.\")\n",
    "        start_time = time.time()\n",
    "        skb = SelectKBest(mutual_info_classif, k=k)\n",
    "        fit = skb.fit(x_train, y_train)\n",
    "        selected_features = fit.get_support(indices=True)\n",
    "        x_train = x_train[x_train.columns[selected_features]]\n",
    "        x_test = x_test[x_test.columns[selected_features]]\n",
    "        end_time = time.time()\n",
    "        feature_selection_time = round(end_time - start_time, 2)\n",
    "        print(\"[Feature Selection] SelectKBest took\", feature_selection_time, \"seconds\")\n",
    "    else:\n",
    "        print(\"[Feature Selection] No feature selection method specified. Using all features.\")\n",
    "    print(\"[Feature Selection] Train dataset shape after feature selection:\", x_train.shape)\n",
    "    print(\"[Feature Selection] Test dataset shape after feature selection:\", x_test.shape)\n",
    "    return x_train, x_test, feature_selection_time, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwargs):\n",
    "    task = kwargs.get(\"task\", \"multi\")\n",
    "    if task not in [\"multi\", \"binary\"]:\n",
    "        raise ValueError(\"Invalid task. Use 'multi' or 'binary'.\")\n",
    "\n",
    "    # Load dataset\n",
    "    train_data, test_data = load_dataset()\n",
    "    train_data = cat_to_num(train_data)\n",
    "    test_data = cat_to_num(test_data)\n",
    "    print(\"[Dataset] Train dataset shape:\", train_data.shape)\n",
    "    print(\"[Dataset] Test dataset shape:\", test_data.shape)\n",
    "    corr_matrix = train_data.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    y_train = train_data[\"attack_cat\"] if task == \"multi\" else train_data[\"label\"]\n",
    "    x_train = train_data.drop([\"id\", \"label\", \"attack_cat\"] + to_drop, axis=1)\n",
    "    y_test = test_data[\"attack_cat\"] if task == \"multi\" else test_data[\"label\"]\n",
    "    x_test = test_data.drop([\"id\", \"label\", \"attack_cat\"] + to_drop, axis=1)\n",
    "    print(\"[Dataset] Train dataset shape after dropping highly correlated features:\", x_train.shape)\n",
    "    print(\"[Dataset] Test dataset shape after dropping highly correlated features:\", x_test.shape)\n",
    "\n",
    "    # Normalize data\n",
    "    x_train = normalize_data(x_train)\n",
    "    x_test = normalize_data(x_test)\n",
    "    k = kwargs.get(\"k\", None)\n",
    "    x_train, x_test, fs_time, k = feature_selection(x_train, y_train, x_test, kwargs.get(\"method\", None), float(k) if k else None)\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    if kwargs.get(\"pca\", None):\n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
    "        X_test = pd.DataFrame(scaler.transform(x_test))\n",
    "        pca = PCA(n_components=int(kwargs.get(\"pca\")) if kwargs.get(\"pca\") else x_train.shape[1])\n",
    "        x_train = pd.DataFrame(pca.fit_transform(x_train))\n",
    "        x_test = pd.DataFrame(pca.transform(X_test))\n",
    "        print(\"[PCA] Train dataset shape after PCA:\", x_train.shape)\n",
    "        print(\"[PCA] Test dataset shape after PCA:\", X_test.shape)\n",
    "    if kwargs.get(\"model_path\", None):\n",
    "        # Load model from local file\n",
    "        model = load_model_from_pickle(kwargs.get(\"model_path\"))\n",
    "        print(\"[Model] Model loaded from\", kwargs.get(\"model_path\"))\n",
    "        model_features = model.get_booster().feature_names\n",
    "        y_pred = model.predict(x_test[model_features])\n",
    "        verbose_output = False\n",
    "    else:\n",
    "        # Model training\n",
    "        grid_search = True if kwargs.get(\"grid_search\", \"false\") == \"true\" else False\n",
    "        if grid_search:\n",
    "            print(\"[Model] Using GridSearchCV to find best hyperparameters\")\n",
    "            xgboost_params = {\n",
    "                \"objective\": [\"multi:softmax\"] if task == \"multi\" else [\"reg:squaredlogerror\"],\n",
    "                \"eval_metric\": [\"mlogloss\", \"auc\"] if task == \"multi\" else [\"logloss\", \"auc\"],\n",
    "                \"min_child_weight\": [1],\n",
    "                \"max_depth\": [6, 8],\n",
    "                \"num_class\": [10] if task == \"multi\" else [1],\n",
    "                \"learning_rate\": [0.01, 0.3],\n",
    "                \"n_estimators\": [100, 1000],\n",
    "                \"subsample\": [0.8],\n",
    "                \"colsample_bytree\": [0.8],\n",
    "            }\n",
    "            model = XGBClassifier()\n",
    "            grid = GridSearchCV(model, xgboost_params, cv=StratifiedKFold(n_splits=5), n_jobs=5, verbose=2, scoring=\"accuracy\")\n",
    "            start_time = time.time()\n",
    "            print(\"[Model] Training started\")\n",
    "            grid.fit(x_train, y_train)\n",
    "            end_time = time.time()\n",
    "            model_training_time = round(end_time - start_time, 2)\n",
    "            print(\"[Model] Training time:\", model_training_time, \"seconds\")\n",
    "            print(\"[Model] Best parameters found by GridSearchCV:\", grid.best_params_)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            xgboost_params = {\n",
    "                \"objective\": \"multi:softmax\" if task == \"multi\" else \"reg:squaredlogerror\",\n",
    "                \"eval_metric\": \"mlogloss\" if task == \"multi\" else \"logloss\",\n",
    "                \"min_child_weight\": 1,\n",
    "                \"max_depth\": 6,\n",
    "                \"num_class\": 10 if task == \"multi\" else 1,\n",
    "                \"learning_rate\": 0.3,\n",
    "                \"n_estimators\": 100,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "            }\n",
    "            model = XGBClassifier(**xgboost_params)\n",
    "            start_time = time.time()\n",
    "            print(\"[Model] Training started\")\n",
    "            model.fit(x_train, y_train)\n",
    "            end_time = time.time()\n",
    "            model_training_time = round(end_time - start_time, 2)\n",
    "            print(\"[Model] Training time:\", model_training_time, \"seconds\")\n",
    "        y_pred = model.predict(x_test)\n",
    "        verbose_output = True\n",
    "    timestamp = int(time.time())\n",
    "    if not kwargs.get(\"model_path\", None):\n",
    "        # Freeze model to disk\n",
    "        print(\"[Model] Freezing params to disk\")\n",
    "        model_name = model.__class__.__name__.lower()\n",
    "        file_prefix = str(timestamp) + \"_\" + model_name + \"_\" + kwargs.get(\"method\", \"none\") + \"_\"\n",
    "        file_prefix += (str(k) if k else \"all\") + \"_\" + task\n",
    "        joblib.dump(model, constants.get(\"CONSTANTS\", \"MODELS_PATH\") + file_prefix + \"_model\" + \".pkl\")\n",
    "        print(\"[I/O] Model saved to\", constants.get(\"CONSTANTS\", \"MODELS_PATH\") + file_prefix + \".pkl\")\n",
    "        file_prefix += str(k) if k else \"all\"\n",
    "        joblib.dump(model, constants.get(\"CONSTANTS\", \"MODELS_PATH\") + file_prefix + \"_model\" + \".pkl\")\n",
    "        print(\"[Model] Model saved to\", constants.get(\"CONSTANTS\", \"MODELS_PATH\") + file_prefix + \".pkl\")\n",
    "    else:\n",
    "        file_prefix = str(timestamp) + \"_loaded_model\"\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"[Model] Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if verbose_output:\n",
    "        report_df.loc[\"\"] = \"\"\n",
    "        report_df.loc[\"model_name\"] = model_name\n",
    "        report_df.loc[\"task\"] = task\n",
    "        report_df.loc[\"pca\"] = kwargs.get(\"pca\", \"none\")\n",
    "        report_df.loc[\"model_training_time\"] = model_training_time\n",
    "        report_df.loc[\"feature_selection_method\"] = kwargs.get(\"method\", \"none\")\n",
    "        report_df.loc[\"k\"] = k\n",
    "        report_df.loc[\"feature_selection_time\"] = fs_time\n",
    "        report_df.loc[\"accuracy\"] = accuracy\n",
    "        report_df.loc[\"recall\"] = report_df.loc[\"weighted avg\"][\"recall\"]\n",
    "        report_df.loc[\"precision\"] = report_df.loc[\"weighted avg\"][\"precision\"]\n",
    "        report_df.loc[\"f1_score\"] = report_df.loc[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    # Save report to disk\n",
    "    report_df.to_csv(\"reports/\" + file_prefix + \"_report.csv\")\n",
    "    print(\"[I/O] Report saved to\", \"reports/\" + file_prefix + \"_report.csv\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, x_train.columns[sorted_idx])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.savefig(\"figures/\" + file_prefix + \"_importance.png\")\n",
    "    print(\"[I/O] Feature importance plot saved to\", \"figures/\" + file_prefix + \"_importance.png\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=np.unique(y_test))\n",
    "    cm_display.plot(cmap=\"Oranges\")\n",
    "    plt.savefig(\"figures/\" + file_prefix + \"_confusion_matrix.png\")\n",
    "    print(\"[I/O] Confusion matrix plot saved to\", \"figures/\" + file_prefix + \"_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(**dict(arg.split(\"=\") for arg in sys.argv[1:]) if len(sys.argv) > 1 else {})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
